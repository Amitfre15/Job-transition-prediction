{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f10f9f2-8cbc-437a-b766-ceb0ed019413",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40920183-31d0-47fd-99f3-f0a09de57057",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc6717e0-e9b6-4930-9aaf-0a2add1963e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading linkedin tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85e6742-18a6-49f7-a681-7f4242bbff53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/linkedin/people')\n",
    "companies = spark.read.parquet('/linkedin/companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e4223d-92b8-445d-9752-2f2325cbf473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "company_reviews = spark.createDataFrame(pd.read_csv(\"/dbfs/FileStore/g45/company_reviews.csv\"))\n",
    "company_reviews = company_reviews.select(\"name\", \"industry\", \"rating\", \"happiness\", \"ratings\", \"roles\", \"employees\")\n",
    "display(company_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a954592-3282-42fb-a18e-3290d7605b40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Map industries to meta-industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480b6634-1ac5-40e5-bc1b-e1c48ce93867",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_industries_12 = {\n",
    "    # Media and Entertainment\n",
    "    # Real Estate and Construction\n",
    "    # Manufacturing\n",
    "    # Retail and Consumer Goods\n",
    "    # Healthcare and Medical\n",
    "    # Education and Training\n",
    "    # Financial and Investment\n",
    "    # Services\n",
    "    # Transportation and Logistics\n",
    "    # Government and Public Policy\n",
    "    # Technology\n",
    "    # Miscellaneous\n",
    "    # Added industries\n",
    "    \"Hotels & Travel Accomodation\": \"Services\",\n",
    "    \"Computers and Eleoctronics\": \"Technology\",\n",
    "    \"Telecommunications\": \"Technology\",\n",
    "    \"Healthcare\": \"Healthcare and Medical\",\n",
    "    \"Financial Services\": \"Financial and Investment\",\n",
    "    \"Manufacturing\": \"Manufacturing\",\n",
    "    \"Education\": \"Education and Training\",\n",
    "    \"Transportation & Logistics\": \"Transportation and Logistics\",\n",
    "    \"Human Resources & Staffing\": \"Services\",\n",
    "    \"Human Resources and Staffing\": \"Services\",\n",
    "    \"Information Technology\": \"Technology\",\n",
    "    \"Real Estate\": \"Real Estate and Construction\",\n",
    "    \"Energy, Mining & Utilities\": \"Miscellaneous\",\n",
    "    \"Consulting and Business Services\": \"Services\",\n",
    "    \"Restaurants, Travel and Leisure\": \"Services\",\n",
    "    \"Restaurants & Food Service\": \"Services\",\n",
    "    \"Construction & Facilities Services\": \"Real Estate and Construction\",\n",
    "    \"Retail & Wholesale\": \"Retail and Consumer Goods\",\n",
    "    \"Nonprofit & NGO\": \"Miscellaneous\",\n",
    "    \"Food and Beverages\": \"Retail and Consumer Goods\",\n",
    "    \"Consumer Goods and Services\": \"Retail and Consumer Goods\",\n",
    "    \"Aerospace & Defense\": \"Manufacturing\",\n",
    "    \"Media & Communication\": \"Media and Entertainment\",\n",
    "    \"Pharmaceutical & Biotechnology\": \"Healthcare and Medical\",\n",
    "    \"Government & Public Administration\": \"Government and Public Policy\",\n",
    "    \"Agriculture and Extraction\": \"Miscellaneous\",\n",
    "    \"Personal Consumer Services\": \"Services\",\n",
    "    \"Legal\": \"Services\",\n",
    "    \"Arts, Entertainment & Recreation\": \"Media and Entertainment\",\n",
    "    \"Health Care\": \"Healthcare and Medical\",\n",
    "    \"Education and Schools\": \"Education and Training\",\n",
    "    \"Internet and Software\": \"Technology\",\n",
    "    \"Agriculture\": \"Miscellaneous\",\n",
    "    \"Construction\": \"Real Estate and Construction\",\n",
    "    \"Industrial Manufacturing\": \"Manufacturing\",\n",
    "    \"Management & Consulting\": \"Services\",\n",
    "    # Additional mappings based on variations and combinations\n",
    "    \"Automotive\": \"Manufacturing\",  # Adjusted to \"Manufacturing\" for clearer context\n",
    "    \"Auto\": \"Manufacturing\",\n",
    "    \"Transport and Freight\": \"Transportation and Logistics\",\n",
    "    \"Retail\": \"Retail and Consumer Goods\",\n",
    "    \"Pharmaceuticals\": \"Healthcare and Medical\",\n",
    "    'Insurance': 'Services'\n",
    "}\n",
    "\n",
    "\n",
    "meta_industry = udf(lambda x: meta_industries_12.get(x))\n",
    "company_reviews = company_reviews.filter(company_reviews.industry.isNotNull())\n",
    "company_reviews = company_reviews.withColumn('meta_industry', meta_industry(F.col('industry')))\n",
    "company_reviews = company_reviews.drop('industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f39326f-edfb-4031-89c6-d7566feeec6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Convert columns with numeric values in json format to float  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c675986-465d-46a9-b471-449ccabf832d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calc_avg_in_json(json_col):\n",
    "    if json_col is None:\n",
    "        return None\n",
    "    try:\n",
    "        # Attempt to replace single quotes with double quotes for JSON parsing\n",
    "        # json_col = str(json_col)\n",
    "        json_col = json_col.replace(\"'\", '\"')\n",
    "        val_dict = json.loads(json_col)\n",
    "        num_values = len(val_dict.values())\n",
    "        return round(sum(float(v) for v in val_dict.values()) / num_values, 1) if num_values > 0 else \"null\"\n",
    "        # return val_dict\n",
    "    except Exception as e:\n",
    "        # For better diagnostics, you could log the error message or return it\n",
    "        return 'Error in parsing: ' + str(e)  # Returning the error message for debugging\n",
    "\n",
    "# Define a UDF to compute the average of numerical values in a dictionary\n",
    "# compute_average = udf(lambda d: sum(float(v) for v in d.values()) / len(d.values()), FloatType())\n",
    "calc_avg_udf = udf(calc_avg_in_json, FloatType())\n",
    "\n",
    "# Apply the UDF to the \"happiness\" column\n",
    "company_reviews = company_reviews.withColumn(\"happiness\", calc_avg_udf(company_reviews[\"happiness\"]))\n",
    "\n",
    "# Apply the UDF to the \"ratings\" column\n",
    "company_reviews = company_reviews.withColumn(\"ratings\", calc_avg_udf(company_reviews[\"ratings\"]))\n",
    "\n",
    "# Apply the UDF to the \"roles\" column\n",
    "company_reviews = company_reviews.withColumn(\"roles\", calc_avg_udf(company_reviews[\"roles\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c857003d-ffd6-4b48-b2f0-35d4dc84fc21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Merge two similar columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267a051f-dcfe-4f5f-8abc-08b14f01475d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate python code to update \"rating\" values based on conditions and drop \"ratings\" column\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "company_reviews = company_reviews.withColumn(\"rating\", \n",
    "                                             F.when(col(\"rating\").isNotNull() & col(\"ratings\").isNotNull(),\n",
    "                                                  (col(\"rating\") + col(\"ratings\")) / 2)\n",
    "                                             .when(col(\"rating\").isNull(), col(\"ratings\"))\n",
    "                                             .otherwise(col(\"rating\")))\n",
    "company_reviews = company_reviews.withColumn(\"rating\", F.round(col(\"rating\"), 2) )\n",
    "\n",
    "company_reviews = company_reviews.drop(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201f5a94-7eef-47b8-9413-51a655870a98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(company_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19470952-170e-47b8-8a21-9e9aed552165",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64454050-381a-4381-a76d-d64eac1d7ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Manipulating \"experience\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c733ec-432f-4d18-9f1b-15ca4093b2ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_profiles = profiles.filter((F.col(\"experience\").isNotNull()) & (F.size(\"experience\") > 0)).select(\"name\", \"experience\", \"education\")\n",
    "exploded_df = filtered_profiles.withColumn(\"experience_item\", F.explode(\"experience\"))\n",
    "atleast_one_trans = exploded_df.groupBy(\"name\").count()\n",
    "# print(f\"n rows {atleast_one_trans.count()}\")\n",
    "atleast_one_trans = atleast_one_trans.filter(F.col(\"count\") > 1)\n",
    "# print(f\"n rows {atleast_one_trans.count()}\")\n",
    "exploded_df = exploded_df.join(atleast_one_trans, on=\"name\", how=\"inner\")\n",
    "# print(f\"n rows {exploded_df.count()}\")\n",
    "# n rows 5712784\n",
    "\n",
    "exploded_df = exploded_df.withColumn(\"exp_title\", F.col(\"experience_item.title\"))\n",
    "exploded_df = exploded_df.withColumn(\"exp_positions\", F.col(\"experience_item.positions\"))\n",
    "exploded_df = exploded_df.withColumn(\"exp_subtitle\", F.col(\"experience_item.subtitle\"))\n",
    "exploded_df = exploded_df.withColumn(\"exp_company_id\", F.col(\"experience_item.company_id\"))\n",
    "exploded_df = exploded_df.withColumn(\"exp_company_name\", F.col(\"experience_item.company\"))\n",
    "exploded_df = exploded_df.withColumn(\"exp_duration\", F.col(\"experience_item.duration\"))\n",
    "exploded_df = exploded_df.withColumn(\"exp_start_date\", F.col(\"experience_item.start_date\"))\n",
    "exploded_df = exploded_df.withColumn(\"exp_end_date\", F.col(\"experience_item.end_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "664437b1-0a1a-4b79-9979-7e9945e2b377",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cleaning invalid records (overlapping employment durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7b582e-9ae8-484a-9d5b-f78036917c17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, when, to_timestamp, lag, expr, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Remove rows with null values in start_date or end_date\n",
    "exploded_df = exploded_df.filter((col(\"exp_start_date\").isNotNull()) & (col(\"exp_end_date\").isNotNull()))\n",
    "\n",
    "# Filter out rows with invalid date formats\n",
    "exploded_df = exploded_df.filter(\n",
    "    ((to_timestamp(\"exp_start_date\", \"MMM yyyy\").isNotNull()) & \n",
    "    (to_timestamp(\"exp_end_date\", \"MMM yyyy\").isNotNull())) |\n",
    "    (col(\"exp_end_date\") == \"Present\")\n",
    ")\n",
    "\n",
    "# # Create an index column\n",
    "window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
    "exploded_df = exploded_df.withColumn(\"index\", F.row_number().over(window_spec))\n",
    "\n",
    "window_spec_name = Window.partitionBy(\"name\").orderBy(\"index\")\n",
    "exploded_df = exploded_df.withColumn(\"present_end_count\", count(when(col(\"exp_end_date\") == \"Present\", True)).over(window_spec_name))\n",
    "# # Add a new column max_present_end_count\n",
    "window_spec_name = Window.partitionBy(\"name\")\n",
    "exploded_df = exploded_df.withColumn(\"max_present_end_count\", F.max(col(\"present_end_count\")).over(window_spec_name))\n",
    "\n",
    "exploded_df = exploded_df.filter(col(\"max_present_end_count\") <= 1)\n",
    "\n",
    "# # Convert exp_start_date and exp_end_date to timestamps\n",
    "exploded_df = exploded_df.withColumn(\"start_date_timestamp\", to_timestamp(\"exp_start_date\", \"MMM yyyy\"))\n",
    "exploded_df = exploded_df.withColumn(\"end_date_timestamp\", \n",
    "                                     when(col(\"exp_end_date\") == \"Present\", current_timestamp())\n",
    "                                     .otherwise(to_timestamp(\"exp_end_date\", \"MMM yyyy\")))\n",
    "\n",
    "# # display(exploded_df)\n",
    "# # print(f\"n rows {exploded_df.count()}\")\n",
    "# # n rows 3175272\n",
    "\n",
    "# Alias exploded_df as e\n",
    "e = exploded_df.alias(\"e\")\n",
    "\n",
    "# Create a copy of exploded_df named higher_ind\n",
    "higher_ind = exploded_df.alias(\"h\")\n",
    "\n",
    "# Join exploded_df and higher_ind based on name column and conditions specified\n",
    "join_condition = ((col(\"e.name\") == col(\"h.name\")) &\n",
    "                  (col(\"e.index\") < col(\"h.index\")) &\n",
    "                  ((col(\"e.start_date_timestamp\") < col(\"h.end_date_timestamp\")) |\n",
    "                   (col(\"e.start_date_timestamp\") < col(\"h.start_date_timestamp\")) |\n",
    "                   (col(\"e.end_date_timestamp\") < col(\"h.start_date_timestamp\")) |\n",
    "                   (col(\"e.end_date_timestamp\") < col(\"h.end_date_timestamp\"))))\n",
    "\n",
    "invalid_df = exploded_df.alias(\"e\").join(higher_ind, join_condition, \"inner\")\n",
    "\n",
    "# Extract the names from the joined DataFrame\n",
    "names_to_remove = invalid_df.select(\"e.name\").distinct()\n",
    "\n",
    "# Filter out rows from exploded_df for the names that appear in names_to_remove\n",
    "exploded_df = exploded_df.join(names_to_remove, exploded_df.name == names_to_remove.name, \"left_anti\")\n",
    "# n rows 1547589\n",
    "display(exploded_df)\n",
    "\n",
    "# # Drop unnecessary columns\n",
    "exploded_df = exploded_df.drop(\"index\", \"exp_start_date\", \"exp_end_date\", \"present_end_count\", \"max_present_end_count\", \"start_date_timestamp\", \"end_date_timestamp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e85b3386-8af3-432b-a3c6-2e90a81f13ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Taking last title from positions column in cases title column is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37ea001-2741-42a1-a44e-416c9035eda5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exploded_df = exploded_df.withColumn(\"exp_title\", F.when(F.col(\"exp_title\").isNull(), F.element_at(F.col(\"exp_positions\"), 1).getField(\"title\")).otherwise(F.col(\"exp_title\")))\n",
    "exploded_df = exploded_df.drop(\"exp_positions\")\n",
    "display(exploded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a985e6d-eb39-40e0-9c59-38f661de5868",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_df = exploded_df.select(\"name\", \"exp_title\", \"exp_subtitle\", \"exp_company_name\", \"exp_company_id\", \"exp_duration\", \"education\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb01985-166c-4f34-bd79-c2f4efbfb9cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Extracting academic degrees from education column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afabdf3-45d9-41b4-8a69-e31b3d5254ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "relevant_df = relevant_df.withColumn(\"education_str\", F.lower(F.col(\"education\").cast(\"string\")))\n",
    "relevant_df = relevant_df.withColumn(\"has_doctor\", F.when(F.col(\"education_str\").like(\"%doctor%\"), 1).when(F.col(\"education_str\").like(\"%[]%\"), None).otherwise(0))\n",
    "relevant_df = relevant_df.withColumn(\"has_master\", F.when((F.col(\"education_str\").like(\"%master%\")) | (F.col(\"has_doctor\") == 1), 1).when(F.col(\"education_str\").like(\"%[]%\"), None).otherwise(0))\n",
    "relevant_df = relevant_df.withColumn(\"has_bachelor\", F.when((F.col(\"education_str\").like(\"%bachelor%\")) | (F.col(\"has_master\") == 1) | (F.col(\"has_doctor\") == 1), 1).when(F.col(\"education_str\").like(\"%[]%\"), None).otherwise(0))\n",
    "relevant_df = relevant_df.drop(\"education\").drop(\"education_str\")\n",
    "display(relevant_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a3c799c-52dd-4d59-9b51-f31fe5fee4bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Making sure each record has a company name in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed78d93f-2289-479e-8fe6-098209f7033c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a new column based on the conditions\n",
    "relevant_df = relevant_df.withColumn(\"final_company_id\",\n",
    "                   F.when(F.col(\"exp_subtitle\").isNotNull(), F.col(\"exp_subtitle\"))\n",
    "                   .when(F.col(\"exp_company_name\").isNotNull(), F.col(\"exp_company_name\"))\n",
    "                #    .when(F.col(\"exp_position_subtitle\").isNotNull(), F.col(\"exp_position_subtitle\"))\n",
    "                   .otherwise(F.col(\"exp_company_id\"))\n",
    "                   )\n",
    "\n",
    "relevant_df = relevant_df.drop(\"exp_subtitle\").drop(\"exp_company_name\").drop(\"exp_company_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8e7f65-acb1-4fd0-911f-1a166d91be39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(relevant_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e465218-d247-47ce-a08c-1b391750cf38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Convert duration column to amount of months in a company "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4de2a3-3baa-44e6-85a1-0de1c1d2b5a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_to_total_months(exp_duration):\n",
    "    years_match = re.search(r'(\\d+)\\s*year', exp_duration)\n",
    "\n",
    "    if exp_duration is None or 'null' in exp_duration.lower():\n",
    "        return None\n",
    "    if 'less than a year' in exp_duration.lower():\n",
    "        return 6\n",
    "    if 'present' in exp_duration.lower():\n",
    "        present_date = datetime.now()\n",
    "        match = re.search(r'(\\w{3}) (\\d{4})', exp_duration)\n",
    "        if match:\n",
    "            start_date = datetime.strptime(match.group(0), '%b %Y')\n",
    "        elif years_match:\n",
    "            years = int(years_match.group(1))\n",
    "            return years * 12\n",
    "        else:\n",
    "            start_date = present_date\n",
    "        \n",
    "        return (present_date.year - start_date.year) * 12 + (present_date.month - start_date.month)\n",
    "    \n",
    "    months_match = re.search(r'(\\d+)\\s*month', exp_duration)\n",
    "    years = int(years_match.group(1)) if years_match else 0\n",
    "    months = int(months_match.group(1)) if months_match else 0\n",
    "    return years * 12 + months\n",
    "\n",
    "# Register UDF\n",
    "convert_to_total_months_udf = udf(convert_to_total_months, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the new column\n",
    "relevant_df = relevant_df.withColumn('exp_months', convert_to_total_months_udf(relevant_df['exp_duration']))\n",
    "relevant_df = relevant_df.drop(\"exp_duration\")\n",
    "display(relevant_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1f56f2-812d-4914-906e-33ff6d80a4bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Applying a company name format before join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfd31c0-3a43-4b23-9ada-70d6534462f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_company_name_before_matching(company_name_df, company_name_col: str):\n",
    "    remove_words = ['llc', 'inc', 'corp', 'corporation', 'ltd', 'limited', 'gmbh', 'pty', 'sarl', 'nv', 'sa']\n",
    "\n",
    "    # Create a regular expression pattern to match these words as whole words\n",
    "    remove_words_pattern = r'\\b(?:' + '|'.join(remove_words) + r')\\b'\n",
    "\n",
    "    # Use the pattern in the regexp_replace function\n",
    "    company_name_df = company_name_df.withColumn(\n",
    "        \"clean_company_name_tmp\",\n",
    "        F.lower(\n",
    "            F.regexp_replace(F.col(company_name_col), \"[^a-zA-Z0-9\\s]+\", \"\")  # Remove punctuation\n",
    "        )\n",
    "    )\n",
    "    company_name_df = company_name_df.withColumn(\n",
    "        \"clean_company_name\",\n",
    "            F.regexp_replace(F.col(\"clean_company_name_tmp\"), remove_words_pattern, \"\")  # Remove specific whole words\n",
    "    )\n",
    "    return company_name_df.drop(\"clean_company_name_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b612169-2409-4bd3-a57f-135868485482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean final_company_id\n",
    "relevant_df = clean_company_name_before_matching(company_name_df=relevant_df, company_name_col=\"final_company_id\")\n",
    "relevant_df = relevant_df.drop(\"final_company_id\")\n",
    "company_reviews = clean_company_name_before_matching(company_name_df=company_reviews, company_name_col=\"name\")\n",
    "company_reviews = company_reviews.drop(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45be58bd-d098-4449-b5e0-8c8b4dc9aa2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(relevant_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360a0e56-9ca4-480f-a527-f7bf221d96c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(company_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ec78d5a-ade1-4bb8-ba33-0b143364efa0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Join the datasets and keep only records with at least one company transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9ec62b-24f8-4d1c-acfb-0e2437122652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = relevant_df.join(company_reviews, on=\"clean_company_name\", how=\"inner\")\n",
    "# print(f\"n rows: {joined_df.count()}\")\n",
    "atleast_one_trans = joined_df.groupBy(\"name\", \"clean_company_name\").count().groupBy(\"name\").count()\n",
    "atleast_one_trans = atleast_one_trans.withColumnRenamed(\"count\", f\"company_count\")\n",
    "# print(f\"n rows {atleast_one_trans.count()}\")\n",
    "atleast_one_trans = atleast_one_trans.filter(F.col(\"company_count\") > 1)\n",
    "# print(f\"n rows {atleast_one_trans.count()}\")\n",
    "joined_df = joined_df.join(atleast_one_trans, on=\"name\", how=\"inner\")\n",
    "n_rows = joined_df.count()\n",
    "# n rows 512573\n",
    "print(f\"n rows {n_rows}\")\n",
    "display(joined_df)\n",
    "joined_df = joined_df.drop(\"company_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c81e38f7-8db3-4897-8833-86499d8617ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Merge multiple records of a person in the same company "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98353560-f31b-47d7-b454-8eecdfdc3ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# Define a window specification to partition by 'name' and 'clean_company_name'\n",
    "window_spec = Window.partitionBy('name', 'clean_company_name')\n",
    "\n",
    "# Add a new column 'exp_months_sum' that calculates the sum of 'exp_months' within the window\n",
    "joined_df = joined_df.withColumn('exp_months', F.sum('exp_months').over(window_spec))\n",
    "\n",
    "# Remove the duplicate rows with the same 'name' and 'clean_company_name', keeping the row with the highest 'exp_months_sum'\n",
    "joined_df = joined_df.withColumn('row_number', F.row_number().over(window_spec.orderBy(F.desc('exp_months')))) \\\n",
    "                     .filter(F.col('row_number') == 1) \\\n",
    "                     .drop('row_number')\n",
    "\n",
    "# Display the updated joined_df dataframe\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3961fb7d-282f-4a40-8337-cf94e18a6751",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating features based on a person's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cccebff-b7a3-4cb4-b633-0df26129db2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correct the last_exp column creation\n",
    "joined_df = joined_df.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "\n",
    "# Create a window specification for collecting previous exp_months\n",
    "exp_history_window = Window.partitionBy(\"name\").orderBy(F.desc(\"index\")).rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "# Create the exp_history column\n",
    "joined_df = joined_df.withColumn(\"exp_history\", F.collect_list(\"exp_months\").over(exp_history_window))\n",
    "joined_df = joined_df.withColumn(\"company_count\", F.size(\"exp_history\"))\n",
    "joined_df = joined_df.withColumn(\"total_exp\", F.sum(\"exp_months\").over(exp_history_window))\n",
    "joined_df = joined_df.withColumn(\"avg_exp\", F.col(\"total_exp\") / F.col(\"company_count\"))\n",
    "\n",
    "exp_history_window = Window.partitionBy(\"name\").orderBy(F.desc(\"index\"))\n",
    "joined_df = joined_df.withColumn(\"last_exp\", F.lag(\"exp_months\").over(exp_history_window))\n",
    "joined_df = joined_df.fillna({\"total_exp\": 0, \"avg_exp\": 0, \"last_exp\": 0})\n",
    "\n",
    "joined_df = joined_df.drop(\"index\").drop(\"exp_history\")\n",
    "\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923a608d-ffb1-467d-8f0e-55ce6ab6610d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = joined_df.fillna(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6628ff18-9a9e-44a5-8f36-6f3d5c382d98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving the dataframe to enable reading it from other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3bf9e4-6cb3-45ad-b93f-aaa08b4960af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Parquet file, and overwrite if it already exists\n",
    "joined_df.write.mode(\"overwrite\").parquet(\"/FileStore/g45/joined_df.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project - trials",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
